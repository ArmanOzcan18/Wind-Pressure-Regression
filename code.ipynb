{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below contain the code for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_filename(file_name):\n",
    "    # This regular expression matches 'Case' followed by any characters until '.mat'\n",
    "    match = re.search(r'(Case.*?)(?=\\.mat)', file_name)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return 'No match found'\n",
    "\n",
    "# Example usage\n",
    "file_name = 'data/TF_204n_Case1_Baseline_RoofOnly_021perc_000deg_Test06.mat'\n",
    "trimmed_name = trim_filename(file_name)\n",
    "print(trimmed_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from scipy.io import loadmat\n",
    "import mat73\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.io import loadmat, savemat\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ConstantKernel as C\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import UndefinedMetricWarning, NotFittedError\n",
    "import matlab.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_0 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_000deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_0)\n",
    "df_0 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_0.shape)\n",
    "\n",
    "file_name_15 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_015deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_15)\n",
    "df_15 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_15.shape)\n",
    "\n",
    "file_name_30 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_030deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_30)\n",
    "df_30 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_30.shape)\n",
    "\n",
    "file_name_45 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_045deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_45)\n",
    "df_45 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_45.shape)\n",
    "\n",
    "file_name_60 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_060deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_60)\n",
    "df_60 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_60.shape)\n",
    "\n",
    "file_name_75 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_075deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_75)\n",
    "df_75 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_75.shape)\n",
    "\n",
    "file_name_90 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_090deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_90)\n",
    "df_90 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_90.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MATLAB engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "used_blocks_number = 1\n",
    "total_block_number = 2\n",
    "\n",
    "def compute_min_peaks(df, num_taps, eng):\n",
    "    results = [ None for _ in range(num_taps) ]\n",
    "\n",
    "    # Iterate through each tap\n",
    "    for tap in range(num_taps):\n",
    "        y_true_1 = df.iloc[ : int(36279/2), tap]\n",
    "        data_dict_1 = {'CP': y_true_1.to_numpy().reshape(-1, 1)}\n",
    "        savemat('output_file_1.mat', data_dict_1)\n",
    "        Cp_min_1, Cp_mean_1, Cp_std_1 = eng.Peaks('output_file_1.mat', nargout=3)\n",
    "        y_true_2 = df.iloc[ int(36279/2):, tap]\n",
    "        data_dict_2 = {'CP': y_true_2.to_numpy().reshape(-1, 1)}\n",
    "        savemat('output_file_2.mat', data_dict_2)\n",
    "        Cp_min_2, Cp_mean_2, Cp_std_2 = eng.Peaks('output_file_2.mat', nargout=3)\n",
    "        \n",
    "        results[tap] = ((Cp_min_1, Cp_mean_1, Cp_std_1), (Cp_min_2, Cp_mean_2, Cp_std_2))\n",
    "\n",
    "    # Convert the results array to a DataFrame\n",
    "    df_min_peaks = pd.DataFrame(results).transpose()\n",
    "    return df_min_peaks\n",
    "\n",
    "# Parameters\n",
    "\n",
    "num_taps = 150  # Assuming there are 150 taps\n",
    "\n",
    "# Compute min peaks for each DataFrame\n",
    "df_stats_0 = compute_min_peaks(df_0, num_taps, eng)\n",
    "df_stats_15 = compute_min_peaks(df_15, num_taps, eng)\n",
    "df_stats_30 = compute_min_peaks(df_30, num_taps, eng)\n",
    "df_stats_45 = compute_min_peaks(df_45, num_taps, eng)\n",
    "df_stats_60 = compute_min_peaks(df_60, num_taps, eng)\n",
    "df_stats_75 = compute_min_peaks(df_75, num_taps, eng)\n",
    "df_stats_90 = compute_min_peaks(df_90, num_taps, eng)\n",
    "\n",
    "df_stats_0.to_pickle('df_entire_double_stats_0.pkl')\n",
    "df_stats_15.to_pickle('df_entire_double_stats_15.pkl')\n",
    "df_stats_30.to_pickle('df_entire_double_stats_30.pkl')\n",
    "df_stats_45.to_pickle('df_entire_double_stats_45.pkl')\n",
    "df_stats_60.to_pickle('df_entire_double_stats_60.pkl')\n",
    "df_stats_75.to_pickle('df_entire_double_stats_75.pkl')\n",
    "df_stats_90.to_pickle('df_entire_double_stats_90.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import savemat\n",
    "import matlab.engine\n",
    "\n",
    "# Start MATLAB engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "used_blocks_number = 1\n",
    "total_block_number = 1\n",
    "\n",
    "def compute_min_peaks(df, num_taps, eng):\n",
    "    results = [ None for _ in range(num_taps) ]\n",
    "\n",
    "    # Iterate through each tap\n",
    "    for tap in range(num_taps):\n",
    "        y_true = df.iloc[ : , tap]\n",
    "        data_dict = {'CP': y_true.to_numpy().reshape(-1, 1)}\n",
    "        savemat('output_file.mat', data_dict)\n",
    "        Cp_min, Cp_mean, Cp_std = eng.Peaks('output_file.mat', nargout=3)\n",
    "        results[tap] = (Cp_min, Cp_mean, Cp_std)\n",
    "\n",
    "    # Convert the results array to a DataFrame\n",
    "    df_min_peaks = pd.DataFrame(results).transpose()\n",
    "    return df_min_peaks\n",
    "\n",
    "# Parameters\n",
    "\n",
    "num_taps = 150  # Assuming there are 150 taps\n",
    "\n",
    "# Compute min peaks for each DataFrame\n",
    "df_stats_0 = compute_min_peaks(df_0, num_taps, eng)\n",
    "df_stats_15 = compute_min_peaks(df_15, num_taps, eng)\n",
    "df_stats_30 = compute_min_peaks(df_30, num_taps, eng)\n",
    "df_stats_45 = compute_min_peaks(df_45, num_taps, eng)\n",
    "df_stats_60 = compute_min_peaks(df_60, num_taps, eng)\n",
    "df_stats_75 = compute_min_peaks(df_75, num_taps, eng)\n",
    "df_stats_90 = compute_min_peaks(df_90, num_taps, eng)\n",
    "\n",
    "df_stats_0.to_pickle('df_entire_stats_0.pkl')\n",
    "df_stats_15.to_pickle('df_entire_stats_15.pkl')\n",
    "df_stats_30.to_pickle('df_entire_stats_30.pkl')\n",
    "df_stats_45.to_pickle('df_entire_stats_45.pkl')\n",
    "df_stats_60.to_pickle('df_entire_stats_60.pkl')\n",
    "df_stats_75.to_pickle('df_entire_stats_75.pkl')\n",
    "df_stats_90.to_pickle('df_entire_stats_90.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = ''\n",
    "\n",
    "df_entire_0 = pd.read_pickle(first + 'df_entire_stats_0.pkl')\n",
    "print(df_entire_0.shape)\n",
    "\n",
    "df_entire_15 = pd.read_pickle(first + 'df_entire_stats_15.pkl')\n",
    "print(df_entire_15.shape)\n",
    "\n",
    "df_entire_30 = pd.read_pickle(first + 'df_entire_stats_30.pkl')\n",
    "print(df_entire_30.shape)\n",
    "\n",
    "df_entire_45 = pd.read_pickle(first + 'df_entire_stats_45.pkl')\n",
    "print(df_entire_45.shape)\n",
    "\n",
    "df_entire_60 = pd.read_pickle(first + 'df_entire_stats_60.pkl')\n",
    "print(df_entire_60.shape)\n",
    "\n",
    "df_entire_75 = pd.read_pickle(first + 'df_entire_stats_75.pkl')\n",
    "print(df_entire_75.shape)\n",
    "\n",
    "df_entire_90 = pd.read_pickle(first + 'df_entire_stats_90.pkl')\n",
    "print(df_entire_90.shape)\n",
    "\n",
    "df_entire_double_0 = pd.read_pickle(first + 'df_entire_double_stats_0.pkl')\n",
    "print(df_entire_0.shape)\n",
    "\n",
    "df_entire_double_15 = pd.read_pickle(first + 'df_entire_double_stats_15.pkl')\n",
    "print(df_entire_15.shape)\n",
    "\n",
    "df_entire_double_30 = pd.read_pickle(first + 'df_entire_double_stats_30.pkl')\n",
    "print(df_entire_30.shape)\n",
    "\n",
    "df_entire_double_45 = pd.read_pickle(first + 'df_entire_double_stats_45.pkl')\n",
    "print(df_entire_45.shape)\n",
    "\n",
    "df_entire_double_60 = pd.read_pickle(first + 'df_entire_double_stats_60.pkl')\n",
    "print(df_entire_60.shape)\n",
    "\n",
    "df_entire_double_75 = pd.read_pickle(first + 'df_entire_double_stats_75.pkl')\n",
    "print(df_entire_75.shape)\n",
    "\n",
    "df_entire_double_90 = pd.read_pickle(first + 'df_entire_double_stats_90.pkl')\n",
    "print(df_entire_90.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations of Pressure Taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal coordinates (from left to right as they appear to increase):\n",
    "horizontal_distances = [2.00, 5.20, 8.40, 12.00, 15.60, 20.11, 25.73, 31.15, 36.77, 42.19]\n",
    "\n",
    "# Vertical coordinates (from bottom to top as they appear to increase):\n",
    "vertical_distances = [2.00, 5.20, 8.15, 11.11, 15.60, 20.55, 25.28, 30.58, 34.72, 39.45, 44.40, 48.89, 51.85, 54.80, 58.00]\n",
    "\n",
    "roof_pressure_tap_coordinates = [(x, y) for x in horizontal_distances for y in vertical_distances]\n",
    "\n",
    "# Horizontal coordinates (from left to right as they appear to increase):\n",
    "horizontal_distances = [0.5, 3.78, 7.06, 10.34]\n",
    "\n",
    "# Vertical coordinates (from top to bottom as they appear to increase):\n",
    "vertical_distances = [6.0, 3.25, 0.5]\n",
    "\n",
    "each_pv_pressure_tap_coordinates = [(x, y) for y in vertical_distances for x in horizontal_distances]\n",
    "\n",
    "pv_pressure_tap_coordinates = []\n",
    "\n",
    "# add (44.90, 8.0) to the coordinates for each pv pressure tap\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 44.90 + 0.20, 4), y + 8.0) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 55.94 + 0.20, 4), y + 8.0) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20,4), y + 8.0) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 55.94 + 0.20, 4), y + 17.45) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 17.45) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 26.89) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 39.34) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 45.79) for x, y in each_pv_pressure_tap_coordinates])\n",
    "\n",
    "# take arccos of 6.21 / 6.50 to find the angle\n",
    "pv_angle_radian = np.arccos(6.21 / 6.50)\n",
    "pv_angle_degree = np.degrees(pv_angle_radian)\n",
    "pv_pressure_tap_coordinates[0]\n",
    "\n",
    "# Let's create a function to duplicate and concatenate the lists\n",
    "def create_combined_list(pv_panel_lists):\n",
    "    combined_list = []\n",
    "    for panel_list in pv_panel_lists:\n",
    "        for coordinate in panel_list:\n",
    "            # Add the coordinate twice, once for top (blue dot) and once for bottom (red dot)\n",
    "            combined_list.append(coordinate)  # For the blue dot\n",
    "        for coordinate in panel_list:\n",
    "            # Add the coordinate twice, once for top (blue dot) and once for bottom (red dot)\n",
    "            combined_list.append(coordinate)  # For the blue dot\n",
    "    return combined_list\n",
    "\n",
    "# Now we create the combined list\n",
    "enumerated_pv_pressure_tap_coordinates = create_combined_list(pv_pressure_tap_coordinates)\n",
    "\n",
    "# attach pv_pressure_tap_coordinates and combined list\n",
    "pressure_tap_coordinates = roof_pressure_tap_coordinates + enumerated_pv_pressure_tap_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Completely Unknown Time History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of all different models for a given wind angle and predicted tap\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.model_selection._search')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.model_selection._validation')\n",
    "\n",
    "# Data preparation\n",
    "taps_for_training = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "tap_index = 81\n",
    "\n",
    "taps_for_training = [46, 47, 48, 62, 63, 76, 77, 78]\n",
    "tap_index = 61\n",
    "\n",
    "taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "tap_index = 1\n",
    "\n",
    "df_entire = df_entire_0\n",
    "\n",
    "# Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "train_data = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'min_peak': [df_entire.iloc[0, tap - 1] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[1, tap - 1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[2, tap - 1] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "# Extract data for tap 1 (index 0 because it's zero-indexed)\n",
    "test_data = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "    'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "    'std': [df_entire.iloc[2, tap_index-1]]\n",
    "})\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'NeuralNetwork': MLPRegressor(),\n",
    "    'GaussianProcess': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store predictions, errors, and best parameters\n",
    "best_predictions = {}\n",
    "best_errors = {}\n",
    "best_model_instance = {}\n",
    "best_params = {}\n",
    "model_errors = {model_name: {} for model_name in models}\n",
    "\n",
    "# Perform regression for each statistic and each model\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    y_train = train_data[stat]\n",
    "    y_true = test_data[stat].values[0]\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_pred = None\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_param = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        print(f'Finished training {model_name} for {stat}')\n",
    "\n",
    "        # Use the best model found\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)[0]\n",
    "\n",
    "        percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "        model_errors[model_name][stat] = percentage_error\n",
    "\n",
    "        # Store the best parameters and errors for each model\n",
    "        if model_name not in best_params:\n",
    "            best_params[model_name] = {}\n",
    "        best_params[model_name][stat] = {\n",
    "            'params': grid_search.best_params_,\n",
    "            'error': percentage_error,\n",
    "            'pred': y_pred\n",
    "        }\n",
    "\n",
    "        if percentage_error < min_error:\n",
    "            min_error = percentage_error\n",
    "            best_pred = y_pred\n",
    "            best_model_name = model_name\n",
    "            best_param = grid_search.best_params_\n",
    "\n",
    "    best_predictions[stat] = best_pred\n",
    "    best_errors[stat] = min_error\n",
    "    best_model_instance[stat] = models[best_model_name]\n",
    "\n",
    "# Calculate and display percentage errors\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    print(f'{stat} - True value: {test_data[stat].values[0]}, Predicted value: {best_predictions[stat]}, Percentage error: {best_errors[stat]:.2f}%, Best model: {best_model_instance[stat]}')\n",
    "\n",
    "# Displaying the results in a tabular format\n",
    "results_df = pd.DataFrame({\n",
    "    'Statistic': ['min_peak', 'mean', 'std'],\n",
    "    'True Value': [test_data[stat].values[0] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Predicted Value': [best_predictions[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Percentage Error': [best_errors[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Model': [type(best_model_instance[stat]).__name__ for stat in ['min_peak', 'mean', 'std']]\n",
    "})\n",
    "\n",
    "# Display the DataFrame using Pandas\n",
    "print(results_df)\n",
    "\n",
    "# Print the best parameters and errors for each model\n",
    "for model_name in best_params:\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        print(f'  Statistic: {stat}')\n",
    "        print(f'    Best Params: {best_params[model_name][stat][\"params\"]}')\n",
    "        print(f'    Predicted Value: {best_params[model_name][stat][\"pred\"]}')\n",
    "        print(f'    Percentage Error: {best_params[model_name][stat][\"error\"]:.2f}%')\n",
    "\n",
    "# Create a table showing the percentage error of each statistic for each model\n",
    "error_table = pd.DataFrame(model_errors).T\n",
    "print(\"\\nPercentage Errors for Each Model and Statistic:\")\n",
    "print(error_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for a fixed wind angle and predicted tap\n",
    "\n",
    "taps_for_training = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "tap_index = 81\n",
    "\n",
    "taps_for_training = [46, 47, 48, 62, 63, 76, 77, 78]\n",
    "tap_index = 61\n",
    "\n",
    "taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "tap_index = 1\n",
    "\n",
    "df_entire = df_entire_0\n",
    "\n",
    "# Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "train_data = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'min_peak': [df_entire.iloc[0, tap - 1] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[1, tap - 1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[2, tap - 1] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "# Extract data for tap 62 (index 61 because it's zero-indexed)\n",
    "test_data = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "    'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "    'std': [df_entire.iloc[2, tap_index-1]]\n",
    "})\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100, 150], 'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2], 'max_depth': [2, 3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5, len(taps_for_training)], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10, 1e-12],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'NeuralNetwork': MLPRegressor(),\n",
    "    'GaussianProcess': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store predictions, errors, and best parameters\n",
    "best_predictions = {}\n",
    "best_errors = {}\n",
    "best_model_instance = {}\n",
    "best_params = {}\n",
    "\n",
    "# Perform regression for each statistic and each model\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    y_train = train_data[stat]\n",
    "    y_true = test_data[stat].values[0]\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_pred = None\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_param = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Use the best model found\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)[0]\n",
    "\n",
    "        percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "        # Store the best parameters and errors for each model\n",
    "        if model_name not in best_params:\n",
    "            best_params[model_name] = {}\n",
    "        best_params[model_name][stat] = {\n",
    "            'params': grid_search.best_params_,\n",
    "            'error': percentage_error,\n",
    "            'pred': y_pred\n",
    "        }\n",
    "\n",
    "        if percentage_error < min_error:\n",
    "            min_error = percentage_error\n",
    "            best_pred = y_pred\n",
    "            best_model_name = model_name\n",
    "            best_param = grid_search.best_params_\n",
    "\n",
    "    best_predictions[stat] = best_pred\n",
    "    best_errors[stat] = min_error\n",
    "    best_model_instance[stat] = models[best_model_name]\n",
    "\n",
    "# Calculate and display percentage errors\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    print(f'{stat} - True value: {test_data[stat].values[0]}, Predicted value: {best_predictions[stat]}, Percentage error: {best_errors[stat]:.2f}%, Best model: {best_model_instance[stat]}')\n",
    "\n",
    "# Displaying the results in a tabular format\n",
    "results_df = pd.DataFrame({\n",
    "    'Statistic': ['min_peak', 'mean', 'std'],\n",
    "    'True Value': [test_data[stat].values[0] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Predicted Value': [best_predictions[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Percentage Error': [best_errors[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Model': [type(best_model_instance[stat]).__name__ for stat in ['min_peak', 'mean', 'std']]\n",
    "})\n",
    "\n",
    "# Display the DataFrame using Pandas\n",
    "print(results_df)\n",
    "\n",
    "# Print the best parameters and errors for each model\n",
    "for model_name in best_params:\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        print(f'  Statistic: {stat}')\n",
    "        print(f'    Best Params: {best_params[model_name][stat][\"params\"]}')\n",
    "        print(f'    Predicted Value: {best_params[model_name][stat][\"pred\"]}')\n",
    "        print(f'    Percentage Error: {best_params[model_name][stat][\"error\"]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with a fixed predicted tap for different wind angles with the name and hyperparameters of the best performing model in each case\n",
    "\n",
    "# angles list\n",
    "angles = [0, 15, 30, 45, 60, 75, 90]\n",
    "\n",
    "# Function to extract train and test data\n",
    "def extract_data(df_entire, taps_for_training, tap_index):\n",
    "    train_data = pd.DataFrame({\n",
    "        'tap': taps_for_training,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1] for tap in taps_for_training],\n",
    "        'mean': [df_entire.iloc[1, tap - 1] for tap in taps_for_training],\n",
    "        'std': [df_entire.iloc[2, tap - 1] for tap in taps_for_training]\n",
    "    })\n",
    "\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "        'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "        'std': [df_entire.iloc[2, tap_index-1]]\n",
    "    })\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100, 150], 'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2], 'max_depth': [2, 3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5, len(taps_for_training)], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10, 1e-12],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'NeuralNetwork': MLPRegressor(),\n",
    "    'GaussianProcess': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to store results\n",
    "angle_results = {angle: {} for angle in angles}\n",
    "\n",
    "# Loop through each angle\n",
    "for angle in angles:\n",
    "    print(f'Processing angle: {angle}')\n",
    "    # Update df_entire based on the angle\n",
    "    df_entire = eval(f'df_entire_{angle}')\n",
    "\n",
    "    # Example taps for training and testing; you need to update these as per your requirement\n",
    "\n",
    "    taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "    tap_index = 1\n",
    "\n",
    "    # Extract data\n",
    "    train_data, test_data = extract_data(df_entire, taps_for_training, tap_index)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "    X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "    # Initialize dictionaries to store best results\n",
    "    best_predictions = {}\n",
    "    best_errors = {}\n",
    "    best_model_instance = {}\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "        best_model = None\n",
    "        best_param = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            # Perform grid search with cross-validation\n",
    "            grid_search = GridSearchCV(model, param_grids[model_name], cv=3)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Use the best model found\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            # Store the best parameters and errors for each model\n",
    "            if model_name not in best_params:\n",
    "                best_params[model_name] = {}\n",
    "            best_params[model_name][stat] = {\n",
    "                'params': grid_search.best_params_,\n",
    "                'error': percentage_error,\n",
    "                'pred': y_pred\n",
    "            }\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "                best_param = grid_search.best_params_\n",
    "\n",
    "        best_predictions[stat] = best_pred\n",
    "        best_errors[stat] = min_error\n",
    "        best_model_instance[stat] = models[best_model_name]\n",
    "        best_params[stat] = best_param\n",
    "\n",
    "    # Store results for the current angle\n",
    "    angle_results[angle] = {\n",
    "        'predictions': best_predictions,\n",
    "        'errors': best_errors,\n",
    "        'models': best_model_instance,\n",
    "        'params': best_params\n",
    "    }\n",
    "\n",
    "    # Print percentage errors for each statistic for the current angle\n",
    "    print(f'\\nResults for angle {angle}:')\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        print(f'{stat} - True value: {test_data[stat].values[0]}, Predicted value: {best_predictions[stat]}, Percentage error: {best_errors[stat]:.2f}%, Best model: {type(best_model_instance[stat]).__name__}, Parameters: {best_params[stat]}')\n",
    "\n",
    "# Display the results for all angles in a tabular format\n",
    "results_list = []\n",
    "for angle, results in angle_results.items():\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        results_list.append({\n",
    "            'Angle': angle,\n",
    "            'Statistic': stat,\n",
    "            'True Value': test_data[stat].values[0],\n",
    "            'Predicted Value': results['predictions'][stat],\n",
    "            'Percentage Error': results['errors'][stat],\n",
    "            'Best Model': type(results['models'][stat]).__name__,\n",
    "            'Parameters': results['params'][stat]\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Partially Unknown Time History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for a fixed wind angle and predicted tap\n",
    "\n",
    "tap_index = 1\n",
    "taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "\n",
    "df_entire = df_entire_double_45\n",
    "\n",
    "# Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "train_data_first_block = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'block': [1] * len(taps_for_training),\n",
    "    'min_peak': [df_entire.iloc[0, tap - 1][0] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[0, tap - 1][1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[0, tap - 1][2] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "train_data_second_block = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'block': [2] * len(taps_for_training),\n",
    "    'min_peak': [df_entire.iloc[1, tap - 1][0] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[1, tap - 1][1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[1, tap - 1][2] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "# Include the first block of the test tap in the training set\n",
    "tap_index_first_block = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'block': [1],\n",
    "    'min_peak': [df_entire.iloc[0, tap_index-1][0]],\n",
    "    'mean': [df_entire.iloc[0, tap_index-1][1]],\n",
    "    'std': [df_entire.iloc[0, tap_index-1][2]]\n",
    "})\n",
    "\n",
    "train_data = pd.concat([train_data_first_block, train_data_second_block, tap_index_first_block], ignore_index=True)\n",
    "\n",
    "# Extract data for the test tap (tap_index is 1-based, so we use tap_index-1 for 0-based indexing)\n",
    "test_data = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'block': [2],\n",
    "    'min_peak': [df_entire.iloc[1, tap_index-1][0]],  # Second block of tap 1\n",
    "    'mean': [df_entire.iloc[1, tap_index-1][1]],\n",
    "    'std': [df_entire.iloc[1, tap_index-1][2]]\n",
    "})\n",
    "\n",
    "# Extract training features and labels\n",
    "X_train = train_data[['x', 'y', 'block']]\n",
    "X_test = test_data[['x', 'y', 'block']]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "    'GaussianProcess': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100, 150], 'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2], 'max_depth': [2, 3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5, len(taps_for_training)], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10, 1e-12],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store predictions and errors\n",
    "best_predictions = {}\n",
    "best_errors = {}\n",
    "best_model = {}\n",
    "best_params = {}\n",
    "\n",
    "# Perform regression for each statistic and each model\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    y_train = train_data[stat]\n",
    "    y_true = test_data[stat].values[0]\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_pred = None\n",
    "    best_model_name = None\n",
    "    best_model_params = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=5)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        y_pred = grid_search.predict(X_test_scaled)[0]\n",
    "\n",
    "        percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "        if percentage_error < min_error:\n",
    "            min_error = percentage_error\n",
    "            best_pred = y_pred\n",
    "            best_model_name = model_name\n",
    "            best_model_params = grid_search.best_params_\n",
    "\n",
    "    best_predictions[stat] = best_pred\n",
    "    best_errors[stat] = min_error\n",
    "    best_model[stat] = best_model_name\n",
    "    best_params[stat] = best_model_params\n",
    "\n",
    "# Calculate and display percentage errors\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    print(f'{stat} - True value: {test_data[stat].values[0]}, Predicted value: {best_predictions[stat]}, Percentage error: {best_errors[stat]:.2f}%, Best model: {best_model[stat]}, Best parameters: {best_params[stat]}')\n",
    "\n",
    "# Compute the average percentage error\n",
    "average_error = sum(best_errors.values()) / len(best_errors)\n",
    "print(f'Average Percentage Error: {average_error:.2f}%')\n",
    "\n",
    "# Displaying the results in a tabular format\n",
    "results_df = pd.DataFrame({\n",
    "    'Statistic': ['min_peak', 'mean', 'std'],\n",
    "    'True Value': [test_data[stat].values[0] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Predicted Value': [best_predictions[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Percentage Error': [best_errors[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Model': [best_model[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Parameters': [best_params[stat] for stat in ['min_peak', 'mean', 'std']]\n",
    "})\n",
    "\n",
    "# Display the DataFrame using Pandas\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for a all wind angles and a specific predicted tap\n",
    "\n",
    "# Define angles to iterate over\n",
    "angles = [0, 15, 30, 45, 60, 75, 90]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (50,50), (100,)], 'alpha': [0.0001, 0.001], 'solver': ['adam'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize lists to store errors and best models for each angle\n",
    "angle_errors = []\n",
    "best_models_info = []\n",
    "\n",
    "# Loop through each angle\n",
    "for angle in angles:\n",
    "    # Set the appropriate data frame for the given angle\n",
    "    df_entire = globals()[f'df_entire_double_{angle}']\n",
    "\n",
    "    tap_index = 1\n",
    "    taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "\n",
    "    tap_index = 61\n",
    "    taps_for_training = [46, 47, 48, 62, 63, 76, 77, 78]\n",
    "\n",
    "    tap_index = 81\n",
    "    taps_for_training = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "\n",
    "    # Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "    train_data_first_block = pd.DataFrame({\n",
    "        'tap': taps_for_training,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "        'block': [1] * len(taps_for_training),\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1][0] for tap in taps_for_training],\n",
    "        'mean': [df_entire.iloc[0, tap - 1][1] for tap in taps_for_training],\n",
    "        'std': [df_entire.iloc[0, tap - 1][2] for tap in taps_for_training]\n",
    "    })\n",
    "\n",
    "    train_data_second_block = pd.DataFrame({\n",
    "        'tap': taps_for_training,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "        'block': [2] * len(taps_for_training),\n",
    "        'min_peak': [df_entire.iloc[1, tap - 1][0] for tap in taps_for_training],\n",
    "        'mean': [df_entire.iloc[1, tap - 1][1] for tap in taps_for_training],\n",
    "        'std': [df_entire.iloc[1, tap - 1][2] for tap in taps_for_training]\n",
    "    })\n",
    "\n",
    "    # Include the first block of the test tap in the training set\n",
    "    tap_index_first_block = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'block': [1],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1][0]],\n",
    "        'mean': [df_entire.iloc[0, tap_index-1][1]],\n",
    "        'std': [df_entire.iloc[0, tap_index-1][2]]\n",
    "    })\n",
    "\n",
    "    train_data = pd.concat([train_data_first_block, train_data_second_block, tap_index_first_block], ignore_index=True)\n",
    "\n",
    "    # Extract data for the test tap (tap_index is 1-based, so we use tap_index-1 for 0-based indexing)\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'block': [2],\n",
    "        'min_peak': [df_entire.iloc[1, tap_index-1][0]],  # Second block of tap 1\n",
    "        'mean': [df_entire.iloc[1, tap_index-1][1]],\n",
    "        'std': [df_entire.iloc[1, tap_index-1][2]]\n",
    "    })\n",
    "\n",
    "    # Extract training features and labels\n",
    "    X_train = train_data[['x', 'y', 'block']]\n",
    "    X_test = test_data[['x', 'y', 'block']]\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeRegressor(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "        'GradientBoosting': GradientBoostingRegressor(),\n",
    "        'KNeighbors': KNeighborsRegressor(),\n",
    "        'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "        'GaussianProcess': GaussianProcessRegressor()\n",
    "    }\n",
    "\n",
    "    # Initialize dictionaries to store predictions and errors\n",
    "    best_predictions = {}\n",
    "    best_errors = {}\n",
    "    best_model = {}\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "        best_model_params = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            grid_search = GridSearchCV(model, param_grids[model_name], cv=5)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "            y_pred = grid_search.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "                best_model_params = grid_search.best_params_\n",
    "\n",
    "        best_predictions[stat] = best_pred\n",
    "        best_errors[stat] = min_error\n",
    "        best_model[stat] = best_model_name\n",
    "        best_params[stat] = best_model_params\n",
    "\n",
    "    # Store the errors and best model info for this angle\n",
    "    angle_errors.append({\n",
    "        'Angle': angle,\n",
    "        'Min Peak Error': best_errors['min_peak'],\n",
    "        'Mean Error': best_errors['mean'],\n",
    "        'Std Error': best_errors['std']\n",
    "    })\n",
    "    \n",
    "    best_models_info.append({\n",
    "        'Angle': angle,\n",
    "        'Min Peak Best Model': best_model['min_peak'],\n",
    "        'Min Peak Best Params': best_params['min_peak'],\n",
    "        'Mean Best Model': best_model['mean'],\n",
    "        'Mean Best Params': best_params['mean'],\n",
    "        'Std Best Model': best_model['std'],\n",
    "        'Std Best Params': best_params['std']\n",
    "    })\n",
    "\n",
    "# Create DataFrames to display the errors and best model info for each angle\n",
    "errors_df = pd.DataFrame(angle_errors)\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "\n",
    "# Display the DataFrames using Pandas\n",
    "print(errors_df)\n",
    "print(best_models_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Missing Wind Angle Time History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for a missing wind angle and a specific predicted tap given the time histories for the other wind angles for that tap\n",
    "\n",
    "# Combine the dataframes with the angle information\n",
    "data_frames = [df_entire_0, df_entire_15, df_entire_30, df_entire_45, df_entire_60, df_entire_75, df_entire_90]\n",
    "angles = [0, 15, 30, 45, 60, 75, 90]\n",
    "\n",
    "tap_no = 81\n",
    "\n",
    "# Create a DataFrame to store all results\n",
    "all_results = []\n",
    "\n",
    "# Loop through each angle to be excluded\n",
    "for angle_excluded in angles:\n",
    "    tap_data = pd.DataFrame({\n",
    "        'angle': angles,\n",
    "        'stat_1': [df.iloc[0, tap_no-1] for df in data_frames],\n",
    "        'stat_2': [df.iloc[1, tap_no-1] for df in data_frames],\n",
    "        'stat_3': [df.iloc[2, tap_no-1] for df in data_frames]\n",
    "    })\n",
    "\n",
    "    # Separate data into training (excluding the current angle) and testing (current angle)\n",
    "    train_data = tap_data[tap_data['angle'] != angle_excluded]\n",
    "    test_data = tap_data[tap_data['angle'] == angle_excluded]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[['angle']])\n",
    "    X_test_scaled = scaler.transform(test_data[['angle']])\n",
    "\n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "        'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "        'GradientBoosting': {'n_estimators': [10, 25, 50, 100, 150], 'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2], 'max_depth': [2, 3, 5, 10]},\n",
    "        'KNeighbors': {'n_neighbors': [2, 3, len(angles)-1], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "        'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [3000]},\n",
    "        'GaussianProcess': {\n",
    "            'kernel': [\n",
    "                C(1.0, constant_value_bounds=(1e-2, 1e2)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "                C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "                C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "            ],\n",
    "            'alpha': [1e-2, 1e-5, 1e-10, 1e-12],\n",
    "            'normalize_y': [False, True]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeRegressor(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "        'GradientBoosting': GradientBoostingRegressor(),\n",
    "        'KNeighbors': KNeighborsRegressor(),\n",
    "        'NeuralNetwork': MLPRegressor(),\n",
    "        'GaussianProcess': GaussianProcessRegressor()\n",
    "    }\n",
    "\n",
    "    # Initialize dictionaries to store predictions, errors, and best parameters\n",
    "    best_predictions = {}\n",
    "    best_errors = {}\n",
    "    best_model_instance = {}\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['stat_1', 'stat_2', 'stat_3']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "        best_model = None\n",
    "        best_param = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            # Perform grid search with cross-validation\n",
    "            grid_search = GridSearchCV(model, param_grids[model_name], cv=3)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Use the best model found\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            # Store the best parameters and errors for each model\n",
    "            if model_name not in best_params:\n",
    "                best_params[model_name] = {}\n",
    "            best_params[model_name][stat] = {\n",
    "                'params': grid_search.best_params_,\n",
    "                'error': percentage_error,\n",
    "                'pred': y_pred\n",
    "            }\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "                best_param = grid_search.best_params_\n",
    "\n",
    "        best_predictions[stat] = best_pred\n",
    "        best_errors[stat] = min_error\n",
    "        best_model_instance[stat] = models[best_model_name]\n",
    "\n",
    "        # Append results to the overall results list\n",
    "        all_results.append({\n",
    "            'Excluded Angle': angle_excluded,\n",
    "            'Statistic': stat,\n",
    "            'True Value': y_true,\n",
    "            'Predicted Value': best_pred,\n",
    "            'Percentage Error': min_error,\n",
    "            'Best Model': type(best_model_instance[stat]).__name__\n",
    "        })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display the DataFrame using Pandas\n",
    "print(results_df)\n",
    "\n",
    "# Print the best parameters and errors for each model\n",
    "for model_name in best_params:\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    for stat in ['stat_1', 'stat_2', 'stat_3']:\n",
    "        print(f'  Statistic: {stat}')\n",
    "        print(f'    Best Params: {best_params[model_name][stat][\"params\"]}')\n",
    "        print(f'    Predicted Value: {best_params[model_name][stat][\"pred\"]}')\n",
    "        print(f'    Percentage Error: {best_params[model_name][stat][\"error\"]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking the Importance of Training Taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the importance of each individual training tap by computing the error percentage after removal of that pressure tap\n",
    "\n",
    "# Function to compute the percentage errors and average error percentage for the removal of a specific tap\n",
    "def compute_errors(taps_for_training, tap_index, df_entire, removed_tap):\n",
    "    # Extract data for the specified taps excluding the removed tap\n",
    "    train_taps = [tap for tap in taps_for_training if tap != removed_tap]\n",
    "\n",
    "    train_data = pd.DataFrame({\n",
    "        'tap': train_taps,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in train_taps],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in train_taps],\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1] for tap in train_taps],\n",
    "        'mean': [df_entire.iloc[1, tap - 1] for tap in train_taps],\n",
    "        'std': [df_entire.iloc[2, tap - 1] for tap in train_taps]\n",
    "    })\n",
    "\n",
    "    # Extract data for the test tap\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "        'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "        'std': [df_entire.iloc[2, tap_index-1]]\n",
    "    })\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "    X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeRegressor(),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=100),\n",
    "        'KNeighbors': KNeighborsRegressor(n_neighbors=len(train_taps)),\n",
    "        'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "        'GaussianProcess': GaussianProcessRegressor()\n",
    "    }\n",
    "\n",
    "    best_errors = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "\n",
    "        best_errors[stat] = min_error\n",
    "\n",
    "    average_error = np.mean(list(best_errors.values()))\n",
    "    return average_error\n",
    "\n",
    "# List of taps to use for training\n",
    "taps_for_training = [2, 3, 4, 5, 16, 17, 18, 19, 20, 31, 32, 33, 34, 35, 46, 47, 48, 49, 50, 61, 62, 63, 64, 65]\n",
    "tap_index = 1\n",
    "\n",
    "df_entire = df_entire_45\n",
    "\n",
    "# Dictionary to store the average error for each removed tap\n",
    "tap_errors = {}\n",
    "\n",
    "# Iterate over each training tap and compute the average error when it is removed\n",
    "for tap in taps_for_training:\n",
    "    average_error = compute_errors(taps_for_training, tap_index, df_entire, tap)\n",
    "    tap_errors[tap] = average_error\n",
    "    print(f'Tap {tap} - Average Error: {average_error:.2f}%')\n",
    "\n",
    "# Rank the taps based on the average error\n",
    "ranked_taps = sorted(tap_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the ranking\n",
    "print(\"\\nTap Importance Ranking (Higher average error percentage after removal means more important):\")\n",
    "for rank, (tap, avg_error) in enumerate(ranked_taps, 1):\n",
    "    print(f'Rank {rank}: Tap {tap} - Average Error: {avg_error:.2f}%')\n",
    "\n",
    "# Visualization part\n",
    "x_coords, y_coords = zip(*roof_pressure_tap_coordinates)\n",
    "x_limits = [0, 45]  # Adjust these limits as needed based on your data\n",
    "y_limits = [0, 60]  # Adjust these limits as needed based on your data\n",
    "\n",
    "# Extract the ranked training taps\n",
    "sorted_trained_taps = [tap for tap, _ in ranked_taps]\n",
    "\n",
    "# Filter out the coordinates for sorted training taps\n",
    "filtered_x_coords_train = [x_coords[tap - 1] for tap in sorted_trained_taps]\n",
    "filtered_y_coords_train = [y_coords[tap - 1] for tap in sorted_trained_taps]\n",
    "filtered_errors = [error for _, error in ranked_taps]\n",
    "\n",
    "# Normalize the errors for coloring\n",
    "norm = plt.Normalize(min(filtered_errors), max(filtered_errors))\n",
    "colors = plt.cm.viridis(norm(filtered_errors))\n",
    "\n",
    "# Create the 2D plot with the sorted training taps using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot sorted training taps with colors indicating importance\n",
    "plt.scatter(filtered_x_coords_train, filtered_y_coords_train, color=colors, label='Training Taps', s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index - 1][1]\n",
    "plt.scatter(predicted_x, predicted_y, color='red', label='Predicted Tap', s=100)\n",
    "\n",
    "# Create a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=norm)\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, label='Importance')\n",
    "\n",
    "plt.xlim(x_limits)\n",
    "plt.ylim(y_limits)\n",
    "plt.gca().set_aspect('equal', adjustable='box')  # Set equal scaling on the axes\n",
    "plt.title('Importance of Each Training Tap')\n",
    "plt.grid(True)\n",
    "# Adding labels\n",
    "plt.xlabel('X Location (inches)')\n",
    "plt.ylabel('Y Location (inches)')\n",
    "# Adding legend\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the importance of each individual training tap by computing the error percentage after removal of that pressure tap (for multiple predicted taps separately)\n",
    "\n",
    "# Function to compute the percentage errors and average error percentage for the removal of a specific tap\n",
    "def compute_errors(taps_for_training, tap_index, df_entire, removed_tap):\n",
    "    # Extract data for the specified taps excluding the removed tap\n",
    "    train_taps = [tap for tap in taps_for_training if tap != removed_tap]\n",
    "\n",
    "    train_data = pd.DataFrame({\n",
    "        'tap': train_taps,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in train_taps],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in train_taps],\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1] for tap in train_taps],\n",
    "        'mean': [df_entire.iloc[1, tap - 1] for tap in train_taps],\n",
    "        'std': [df_entire.iloc[2, tap - 1] for tap in train_taps]\n",
    "    })\n",
    "\n",
    "    # Extract data for the test tap\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "        'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "        'std': [df_entire.iloc[2, tap_index-1]]\n",
    "    })\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "    X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeRegressor(),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "        'GradientBoosting': GradientBoostingRegressor(n_estimators=100),\n",
    "        'KNeighbors': KNeighborsRegressor(n_neighbors=len(train_taps)),\n",
    "        'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "        'GaussianProcess': GaussianProcessRegressor()\n",
    "    }\n",
    "\n",
    "    best_errors = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "\n",
    "        best_errors[stat] = min_error\n",
    "\n",
    "    average_error = np.mean(list(best_errors.values()))\n",
    "    return average_error\n",
    "\n",
    "# Taps configurations\n",
    "taps_configurations = {\n",
    "    1: ([2, 3, 4, 5, 16, 17, 18, 19, 20, 31, 32, 33, 34, 35, 46, 47, 48, 49, 50, 61, 62, 63, 64, 65], df_entire_45),\n",
    "    61: ([46, 47, 62, 76, 77, 31, 32, 33, 48, 63, 78, 91, 92, 93, 16, 17, 18, 19, 34, 49, 64, 79, 94, 106, 107, 108, 109], df_entire_0),\n",
    "    81: ([65, 66, 67, 80, 82, 95, 96, 97, 49, 50, 51, 52, 53, 64, 68, 79, 83, 94, 98, 109, 110, 111, 112, 113], df_entire_0)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=True)\n",
    "\n",
    "# Process each configuration\n",
    "for ax, (tap_index, (taps_for_training, df_entire)) in zip(axes, taps_configurations.items()):\n",
    "    tap_errors = {}\n",
    "\n",
    "    # Compute the average error for each tap\n",
    "    for tap in taps_for_training:\n",
    "        average_error = compute_errors(taps_for_training, tap_index, df_entire, tap)\n",
    "        tap_errors[tap] = average_error\n",
    "        print(f'Tap {tap} - Average Error: {average_error:.2f}%')\n",
    "\n",
    "    # Rank the taps based on the average error\n",
    "    ranked_taps = sorted(tap_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Visualization part\n",
    "    x_coords, y_coords = zip(*roof_pressure_tap_coordinates)\n",
    "    x_limits = [0, 45]  # Adjust these limits as needed based on your data\n",
    "    y_limits = [0, 60]  # Adjust these limits as needed based on your data\n",
    "\n",
    "    # Extract the ranked training taps\n",
    "    sorted_trained_taps = [tap for tap, _ in ranked_taps]\n",
    "\n",
    "    # Filter out the coordinates for sorted training taps\n",
    "    filtered_x_coords_train = [x_coords[tap - 1] for tap in sorted_trained_taps]\n",
    "    filtered_y_coords_train = [y_coords[tap - 1] for tap in sorted_trained_taps]\n",
    "    filtered_errors = [error for _, error in ranked_taps]\n",
    "\n",
    "    # Normalize the errors for coloring\n",
    "    norm = plt.Normalize(min(filtered_errors), max(filtered_errors))\n",
    "    colors = plt.cm.viridis(norm(filtered_errors))\n",
    "\n",
    "    # Plot sorted training taps with colors indicating importance\n",
    "    sc = ax.scatter(filtered_x_coords_train, filtered_y_coords_train, color=colors, s=100)\n",
    "\n",
    "    # Plot the predicted tap in red\n",
    "    predicted_x = roof_pressure_tap_coordinates[tap_index - 1][0]\n",
    "    predicted_y = roof_pressure_tap_coordinates[tap_index - 1][1]\n",
    "    ax.scatter(predicted_x, predicted_y, color='red', label='Predicted Tap', s=100)\n",
    "\n",
    "    ax.set_xlim(x_limits)\n",
    "    ax.set_ylim(y_limits)\n",
    "    ax.set_aspect('equal', adjustable='box')  # Set equal scaling on the axes\n",
    "    ax.set_title(f'Tap {tap_index}')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('X Location (inches)')\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel('Y Location (inches)')\n",
    "\n",
    "# Create a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Importance')\n",
    "\n",
    "# Adding legend once for all\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the importance of each layer of training taps by computing the error percentage when only that layer is used in training\n",
    "\n",
    "# Function to compute the percentage errors and average error percentage for the removal of a specific tap\n",
    "def compute_errors(taps_for_training, tap_index, df_entire, removed_tap=None, model_type='all'):\n",
    "    if removed_tap is not None:\n",
    "        train_taps = [tap for tap in taps_for_training if tap != removed_tap]\n",
    "    else:\n",
    "        train_taps = taps_for_training\n",
    "\n",
    "    train_data = pd.DataFrame({\n",
    "        'tap': train_taps,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in train_taps],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in train_taps],\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1] for tap in train_taps],\n",
    "        'mean': [df_entire.iloc[1, tap - 1] for tap in train_taps],\n",
    "        'std': [df_entire.iloc[2, tap - 1] for tap in train_taps]\n",
    "    })\n",
    "\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "        'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "        'std': [df_entire.iloc[2, tap_index-1]]\n",
    "    })\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "    X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "    if model_type == 'all':\n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'DecisionTree': DecisionTreeRegressor(),\n",
    "            'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "            'GradientBoosting': GradientBoostingRegressor(n_estimators=100),\n",
    "            'KNeighbors': KNeighborsRegressor(n_neighbors=len(train_taps)),\n",
    "            'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "            'GaussianProcess': GaussianProcessRegressor()\n",
    "        }\n",
    "    elif model_type == 'DecisionTree':\n",
    "        models = {'DecisionTree': DecisionTreeRegressor()}\n",
    "    elif model_type == 'KNeighbors':\n",
    "        models = {'KNeighbors': KNeighborsRegressor(n_neighbors=len(train_taps))}\n",
    "    elif model_type == 'RandomForest':\n",
    "        models = {'RandomForest': RandomForestRegressor(n_estimators=100)}\n",
    "    elif model_type == 'GradientBoosting':\n",
    "        models = {'GradientBoosting': GradientBoostingRegressor(n_estimators=100)}\n",
    "    elif model_type == 'NeuralNetwork':\n",
    "        models = {'NeuralNetwork': MLPRegressor(max_iter=3000)}\n",
    "    elif model_type == 'GaussianProcess':\n",
    "        models = {'GaussianProcess': GaussianProcessRegressor()}\n",
    "\n",
    "    best_errors = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "\n",
    "        best_errors[stat] = min_error\n",
    "\n",
    "    average_error = np.mean(list(best_errors.values()))\n",
    "    return average_error, best_model_name\n",
    "\n",
    "\n",
    "# Function to compute the errors for each layer separately\n",
    "def compute_layer_errors_separately(first_layer, second_layer, third_layer, tap_index, df_entire):\n",
    "    layer_errors = {}\n",
    "    layers = [first_layer, second_layer, third_layer]\n",
    "    layer_labels = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "\n",
    "    for layer, label in zip(layers, layer_labels):\n",
    "        layer_errors[label], _ = compute_errors(layer, tap_index, df_entire)\n",
    "        print(f'{label} Error: {layer_errors[label]:.2f}%')\n",
    "\n",
    "    return layer_errors\n",
    "\n",
    "# Taps configurations for tap 1\n",
    "taps_for_training_1 = [2, 3, 4, 5, 16, 17, 18, 19, 20, 31, 32, 33, 34, 35, 46, 47, 48, 49, 50, 61, 62, 63, 64, 65]\n",
    "tap_index_1 = 1\n",
    "df_entire_1 = df_entire_45\n",
    "\n",
    "# Taps configurations for tap 61\n",
    "first_layer_61 = [46, 47, 62, 76, 77]\n",
    "second_layer_61 = [31, 32, 33, 48, 63, 78, 91, 92, 93]\n",
    "third_layer_61 = [16, 17, 18, 19, 34, 49, 64, 79, 94, 106, 107, 108, 109]\n",
    "tap_index_61 = 61\n",
    "df_entire_61 = df_entire_0\n",
    "\n",
    "# Taps configurations for tap 81\n",
    "first_layer_81 = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "second_layer_81 = [49, 50, 51, 52, 53, 64, 68, 79, 83, 94, 98, 109, 110, 111, 112, 113]\n",
    "third_layer_81 = [33, 34, 35, 36, 37, 38, 39, 54, 69, 84, 99, 114, 48, 63, 78, 93, 108, 123, 124, 125, 126, 127, 128, 129]\n",
    "tap_index_81 = 81\n",
    "df_entire_81 = df_entire_90\n",
    "\n",
    "\n",
    "# Compute errors for each tap for tap 1\n",
    "tap_errors_1 = {}\n",
    "for tap in taps_for_training_1:\n",
    "    tap_errors_1[tap], _ = compute_errors(taps_for_training_1, tap_index_1, df_entire_1, removed_tap=tap)\n",
    "    print(f'Tap {tap} - Error: {tap_errors_1[tap]:.2f}%')\n",
    "tap_errors_1 = sorted(tap_errors_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Compute errors for each layer separately for tap 61\n",
    "layer_errors_61 = compute_layer_errors_separately(first_layer_61, second_layer_61, third_layer_61, tap_index_61, df_entire_61)\n",
    "\n",
    "# Compute errors for each layer separately for tap 81\n",
    "layer_errors_81 = compute_layer_errors_separately(first_layer_81, second_layer_81, third_layer_81, tap_index_81, df_entire_81)\n",
    "\n",
    "# Prepare the figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=True)\n",
    "\n",
    "# Plot for tap 1\n",
    "ax = axes[0]\n",
    "x_coords, y_coords = zip(*roof_pressure_tap_coordinates)\n",
    "sorted_taps_1, errors_1 = zip(*tap_errors_1)\n",
    "norm = plt.Normalize(min(errors_1), max(errors_1))\n",
    "colors_1 = plt.cm.viridis(norm(errors_1))\n",
    "\n",
    "filtered_x_coords_1 = [x_coords[tap - 1] for tap in sorted_taps_1]\n",
    "filtered_y_coords_1 = [y_coords[tap - 1] for tap in sorted_taps_1]\n",
    "ax.scatter(filtered_x_coords_1, filtered_y_coords_1, color=colors_1, s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index_1 - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index_1 - 1][1]\n",
    "ax.scatter(predicted_x, predicted_y, color='red', s=100)\n",
    "\n",
    "ax.set_xlim(0, 45)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('Tap 1')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('X Location (inches)')\n",
    "ax.set_ylabel('Y Location (inches)')\n",
    "\n",
    "# Plot for tap 61\n",
    "ax = axes[1]\n",
    "layer_colors_61 = [layer_errors_61['Layer 1'], layer_errors_61['Layer 2'], layer_errors_61['Layer 3']]\n",
    "layer_labels_61 = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "layer_taps_61 = [first_layer_61, second_layer_61, third_layer_61]\n",
    "norm = plt.Normalize(min(layer_colors_61), max(layer_colors_61))\n",
    "colors_61 = plt.cm.viridis(norm(layer_colors_61))\n",
    "\n",
    "for taps, color, label in zip(layer_taps_61, colors_61, layer_labels_61):\n",
    "    filtered_x_coords = [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps]\n",
    "    filtered_y_coords = [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps]\n",
    "    ax.scatter(filtered_x_coords, filtered_y_coords, color=color, label=label, s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index_61 - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index_61 - 1][1]\n",
    "ax.scatter(predicted_x,\n",
    "\n",
    " predicted_y, color='red', s=100)\n",
    "\n",
    "ax.set_xlim(0, 45)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('Tap 61')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('X Location (inches)')\n",
    "\n",
    "# Plot for tap 81\n",
    "ax = axes[2]\n",
    "layer_colors_81 = [layer_errors_81['Layer 1'], layer_errors_81['Layer 2'], layer_errors_81['Layer 3']]\n",
    "layer_labels_81 = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "layer_taps_81 = [first_layer_81, second_layer_81, third_layer_81]\n",
    "colors_81 = plt.cm.viridis(norm(layer_colors_81))\n",
    "\n",
    "for taps, color, label in zip(layer_taps_81, colors_81, layer_labels_81):\n",
    "    filtered_x_coords = [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps]\n",
    "    filtered_y_coords = [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps]\n",
    "    ax.scatter(filtered_x_coords, filtered_y_coords, color=color, label=label, s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index_81 - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index_81 - 1][1]\n",
    "ax.scatter(predicted_x, predicted_y, color='red', s=100)\n",
    "\n",
    "ax.set_xlim(0, 45)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('Tap 81')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('X Location (inches)')\n",
    "\n",
    "# Create a colorbar without numbers\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Importance')\n",
    "cbar.ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "# Adding legend once for all\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the importance of each training tap layers by computing the error percentage when each layer is additively included in the set of training taps\n",
    "\n",
    "# Function to compute the percentage errors and average error percentage for the removal of a specific tap\n",
    "def compute_errors(taps_for_training, tap_index, df_entire, removed_tap=None, model_type='all'):\n",
    "    if removed_tap is not None:\n",
    "        train_taps = [tap for tap in taps_for_training if tap != removed_tap]\n",
    "    else:\n",
    "        train_taps = taps_for_training\n",
    "\n",
    "    train_data = pd.DataFrame({\n",
    "        'tap': train_taps,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in train_taps],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in train_taps],\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1] for tap in train_taps],\n",
    "        'mean': [df_entire.iloc[1, tap - 1] for tap in train_taps],\n",
    "        'std': [df_entire.iloc[2, tap - 1] for tap in train_taps]\n",
    "    })\n",
    "\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "        'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "        'std': [df_entire.iloc[2, tap_index-1]]\n",
    "    })\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "    X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "    if model_type == 'all':\n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'DecisionTree': DecisionTreeRegressor(),\n",
    "            'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "            'GradientBoosting': GradientBoostingRegressor(n_estimators=100),\n",
    "            'KNeighbors': KNeighborsRegressor(n_neighbors=len(train_taps)),\n",
    "            'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "            'GaussianProcess': GaussianProcessRegressor()\n",
    "        }\n",
    "    elif model_type == 'DecisionTree':\n",
    "        models = {'DecisionTree': DecisionTreeRegressor()}\n",
    "    elif model_type == 'KNeighbors':\n",
    "        models = {'KNeighbors': KNeighborsRegressor(n_neighbors=len(train_taps))}\n",
    "    elif model_type == 'RandomForest':\n",
    "        models = {'RandomForest': RandomForestRegressor(n_estimators=100)}\n",
    "    elif model_type == 'GradientBoosting':\n",
    "        models = {'GradientBoosting': GradientBoostingRegressor(n_estimators=100)}\n",
    "    elif model_type == 'NeuralNetwork':\n",
    "        models = {'NeuralNetwork': MLPRegressor(max_iter=3000)}\n",
    "    elif model_type == 'GaussianProcess':\n",
    "        models = {'GaussianProcess': GaussianProcessRegressor()}\n",
    "\n",
    "    best_errors = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "\n",
    "        best_errors[stat] = min_error\n",
    "\n",
    "    average_error = np.mean(list(best_errors.values()))\n",
    "    return average_error, best_model_name\n",
    "\n",
    "# Function to compute the errors for each layer additively\n",
    "def compute_layer_errors_additively(first_layer, second_layer, third_layer, tap_index, df_entire):\n",
    "    layer_errors = {}\n",
    "    combined_layers = [first_layer, first_layer + second_layer, first_layer + second_layer + third_layer]\n",
    "    layer_labels = ['Layer 1', 'Layer 1+2', 'Layer 1+2+3']\n",
    "\n",
    "    for taps, label in zip(combined_layers, layer_labels):\n",
    "        layer_errors[label], _ = compute_errors(taps, tap_index, df_entire, model_type='GaussianProcess')\n",
    "        print(f'{label} Error: {layer_errors[label]:.2f}%')\n",
    "\n",
    "    return layer_errors\n",
    "\n",
    "# Taps configurations for tap 1\n",
    "first_layer_taps_1 = [2, 16, 17]\n",
    "second_layer_taps_1 = [3, 18, 31, 32, 33]\n",
    "third_layer_taps_1 = [4, 19, 34, 46, 47, 48, 49]\n",
    "tap_index_1 = 1\n",
    "df_entire_1 = df_entire_90\n",
    "\n",
    "# Taps configurations for tap 61\n",
    "first_layer_61 = [46, 47, 62, 76, 77]\n",
    "second_layer_61 = [31, 32, 33, 48, 63, 78, 91, 92, 93]\n",
    "third_layer_61 = [16, 17, 18, 19, 34, 49, 64, 79, 94, 106, 107, 108, 109]\n",
    "tap_index_61 = 61\n",
    "df_entire_61 = df_entire_90\n",
    "\n",
    "# Taps configurations for tap 81\n",
    "first_layer_81 = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "second_layer_81 = [49, 50, 51, 52, 53, 64, 68, 79, 83, 94, 98, 109, 110, 111, 112, 113]\n",
    "third_layer_81 = [33, 34, 35, 36, 37, 38, 39, 54, 69, 84, 99, 114, 48, 63, 78, 93, 108, 123, 124, 125, 126, 127, 128, 129]\n",
    "tap_index_81 = 81\n",
    "df_entire_81 = df_entire_90\n",
    "\n",
    "# Compute errors for each layer additively for tap 1\n",
    "layer_errors_1 = compute_layer_errors_additively(first_layer_taps_1, second_layer_taps_1, third_layer_taps_1, tap_index_1, df_entire_1)\n",
    "\n",
    "# Compute errors for each layer additively for tap 61\n",
    "layer_errors_61 = compute_layer_errors_additively(first_layer_61, second_layer_61, third_layer_61, tap_index_61, df_entire_61)\n",
    "\n",
    "# Compute errors for each layer additively for tap 81\n",
    "layer_errors_81 = compute_layer_errors_additively(first_layer_81, second_layer_81, third_layer_81, tap_index_81, df_entire_81)\n",
    "\n",
    "# Invert the importance for tap 61 and tap 81 (higher error means lower importance)\n",
    "max_error_61 = max(layer_errors_61.values())\n",
    "min_error_61 = min(layer_errors_61.values())\n",
    "layer_errors_61 = {k: (max_error_61 - v + min_error_61) for k, v in layer_errors_61.items()}\n",
    "\n",
    "max_error_81 = max(layer_errors_81.values())\n",
    "min_error_81 = min(layer_errors_81.values())\n",
    "layer_errors_81 = {k: (max_error_81 - v + min_error_81) for k, v in layer_errors_81.items()}\n",
    "\n",
    "# Prepare the figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), sharey=True)\n",
    "\n",
    "# Plot for tap 1\n",
    "ax = axes[0]\n",
    "x_coords, y_coords = zip(*roof_pressure_tap_coordinates)\n",
    "layer_colors_1 = [layer_errors_1['Layer 1'], layer_errors_1['Layer 1+2'], layer_errors_1['Layer 1+2+3']]\n",
    "layer_labels_1 = ['Layer 1', 'Layer 1+2', 'Layer 1+2+3']\n",
    "layer_taps_1 = [first_layer_taps_1, first_layer_taps_1 + second_layer_taps_1, first_layer_taps_1 + second_layer_taps_1 + third_layer_taps_1]\n",
    "norm = plt.Normalize(min(layer_colors_1), max(layer_colors_1))\n",
    "colors_1 = plt.cm.viridis(norm(layer_colors_1))\n",
    "\n",
    "for taps, color, label in zip(layer_taps_1, colors_1, layer_labels_1):\n",
    "    filtered_x_coords = [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps]\n",
    "    filtered_y_coords = [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps]\n",
    "    ax.scatter(filtered_x_coords, filtered_y_coords, color=color, label=label, s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index_1 - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index_1 - 1][1]\n",
    "ax.scatter(predicted_x, predicted_y, color='red', s=100)\n",
    "\n",
    "ax.set_xlim(0, 45)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('Tap 1')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('X Location (inches)')\n",
    "ax.set_ylabel('Y Location (inches)')\n",
    "\n",
    "# Plot for tap 61\n",
    "ax = axes[1]\n",
    "layer_colors_61 = [layer_errors_61['Layer 1'], layer_errors_61['Layer 1+2'], layer_errors_61['Layer 1+2+3']]\n",
    "layer_labels_61 = ['Layer 1', 'Layer 1+2', 'Layer 1+2+3']\n",
    "layer_taps_61 = [first_layer_61, first_layer_61 + second_layer_61, first_layer_61 + second_layer_61 + third_layer_61]\n",
    "norm = plt.Normalize(min(layer_colors_61), max(layer_colors_61))\n",
    "colors_61 = plt.cm.viridis(norm(layer_colors_61))\n",
    "\n",
    "for taps, color, label in zip(layer_taps_61, colors_61, layer_labels_61):\n",
    "    filtered_x_coords = [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps]\n",
    "    filtered_y_coords = [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps]\n",
    "    ax.scatter(filtered_x_coords, filtered_y_coords, color=color, label=label, s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index_61 - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index_61 - 1][1]\n",
    "ax.scatter(predicted_x, predicted_y, color='red', s=100)\n",
    "\n",
    "ax.set_xlim(0, 45)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('Tap 61')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('X Location (inches)')\n",
    "\n",
    "# Plot for tap 81\n",
    "ax = axes[2]\n",
    "layer_colors_81 = [layer_errors_81['Layer 1'], layer_errors_81['Layer 1+2'], layer_errors_81['Layer 1+2+3']]\n",
    "layer_labels_81 = ['Layer 1', 'Layer 1+2', 'Layer 1+2+3']\n",
    "layer_taps_81 = [first_layer_81, first_layer_81 + second_layer_81, first_layer_81 + second_layer_81 + third_layer_81]\n",
    "colors_81 = plt.cm.viridis(norm(layer_colors_81))\n",
    "\n",
    "for taps, color, label in zip(layer_taps_81, colors_81, layer_labels_81):\n",
    "    filtered_x_coords = [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps]\n",
    "    filtered_y_coords = [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps]\n",
    "    ax.scatter(filtered_x_coords, filtered_y_coords, color=color, label=label, s=100)\n",
    "\n",
    "# Plot the predicted tap in red\n",
    "predicted_x = roof_pressure_tap_coordinates[tap_index_81 - 1][0]\n",
    "predicted_y = roof_pressure_tap_coordinates[tap_index_81 - 1][1]\n",
    "ax.scatter(predicted_x, predicted_y, color='red', s=100)\n",
    "\n",
    "ax.set_xlim(0, 45)\n",
    "ax.set_ylim(0, 60)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_title('Tap 81')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('X Location (inches)')\n",
    "\n",
    "# Create a colorbar without numbers\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Importance')\n",
    "cbar.ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "# Adding legend once for all\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on Pressure Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Completely Unknown Time History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Partially Unknown Time History"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

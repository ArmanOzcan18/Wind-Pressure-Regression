{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below contain the code for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_filename(file_name):\n",
    "    # This regular expression matches 'Case' followed by any characters until '.mat'\n",
    "    match = re.search(r'(Case.*?)(?=\\.mat)', file_name)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return 'No match found'\n",
    "\n",
    "# Example usage\n",
    "file_name = 'data/TF_204n_Case1_Baseline_RoofOnly_021perc_000deg_Test06.mat'\n",
    "trimmed_name = trim_filename(file_name)\n",
    "print(trimmed_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from scipy.io import loadmat\n",
    "import mat73\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.io import loadmat, savemat\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ConstantKernel as C\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import UndefinedMetricWarning, NotFittedError\n",
    "import matlab.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_0 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_000deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_0)\n",
    "df_0 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_0.shape)\n",
    "\n",
    "file_name_15 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_015deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_15)\n",
    "df_15 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_15.shape)\n",
    "\n",
    "file_name_30 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_030deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_30)\n",
    "df_30 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_30.shape)\n",
    "\n",
    "file_name_45 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_045deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_45)\n",
    "df_45 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_45.shape)\n",
    "\n",
    "file_name_60 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_060deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_60)\n",
    "df_60 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_60.shape)\n",
    "\n",
    "file_name_75 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_075deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_75)\n",
    "df_75 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_75.shape)\n",
    "\n",
    "file_name_90 = 'data/TF_204n_Case1_Baseline_RoofOnly_044_5perc_090deg_Test05_Roof_Cp3sec.mat'\n",
    "annots = mat73.loadmat(file_name_90)\n",
    "df_90 = pd.DataFrame(annots['CpRoof'])\n",
    "print(df_90.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MATLAB engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "used_blocks_number = 1\n",
    "total_block_number = 2\n",
    "\n",
    "def compute_min_peaks(df, num_taps, eng):\n",
    "    results = [ None for _ in range(num_taps) ]\n",
    "\n",
    "    # Iterate through each tap\n",
    "    for tap in range(num_taps):\n",
    "        y_true_1 = df.iloc[ : int(36279/2), tap]\n",
    "        data_dict_1 = {'CP': y_true_1.to_numpy().reshape(-1, 1)}\n",
    "        savemat('output_file_1.mat', data_dict_1)\n",
    "        Cp_min_1, Cp_mean_1, Cp_std_1 = eng.Peaks('output_file_1.mat', nargout=3)\n",
    "        y_true_2 = df.iloc[ int(36279/2):, tap]\n",
    "        data_dict_2 = {'CP': y_true_2.to_numpy().reshape(-1, 1)}\n",
    "        savemat('output_file_2.mat', data_dict_2)\n",
    "        Cp_min_2, Cp_mean_2, Cp_std_2 = eng.Peaks('output_file_2.mat', nargout=3)\n",
    "        \n",
    "        results[tap] = ((Cp_min_1, Cp_mean_1, Cp_std_1), (Cp_min_2, Cp_mean_2, Cp_std_2))\n",
    "\n",
    "    # Convert the results array to a DataFrame\n",
    "    df_min_peaks = pd.DataFrame(results).transpose()\n",
    "    return df_min_peaks\n",
    "\n",
    "# Parameters\n",
    "\n",
    "num_taps = 150  # Assuming there are 150 taps\n",
    "\n",
    "# Compute min peaks for each DataFrame\n",
    "df_stats_0 = compute_min_peaks(df_0, num_taps, eng)\n",
    "df_stats_15 = compute_min_peaks(df_15, num_taps, eng)\n",
    "df_stats_30 = compute_min_peaks(df_30, num_taps, eng)\n",
    "df_stats_45 = compute_min_peaks(df_45, num_taps, eng)\n",
    "df_stats_60 = compute_min_peaks(df_60, num_taps, eng)\n",
    "df_stats_75 = compute_min_peaks(df_75, num_taps, eng)\n",
    "df_stats_90 = compute_min_peaks(df_90, num_taps, eng)\n",
    "\n",
    "df_stats_0.to_pickle('df_entire_double_stats_0.pkl')\n",
    "df_stats_15.to_pickle('df_entire_double_stats_15.pkl')\n",
    "df_stats_30.to_pickle('df_entire_double_stats_30.pkl')\n",
    "df_stats_45.to_pickle('df_entire_double_stats_45.pkl')\n",
    "df_stats_60.to_pickle('df_entire_double_stats_60.pkl')\n",
    "df_stats_75.to_pickle('df_entire_double_stats_75.pkl')\n",
    "df_stats_90.to_pickle('df_entire_double_stats_90.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import savemat\n",
    "import matlab.engine\n",
    "\n",
    "# Start MATLAB engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "used_blocks_number = 1\n",
    "total_block_number = 1\n",
    "\n",
    "def compute_min_peaks(df, num_taps, eng):\n",
    "    results = [ None for _ in range(num_taps) ]\n",
    "\n",
    "    # Iterate through each tap\n",
    "    for tap in range(num_taps):\n",
    "        y_true = df.iloc[ : , tap]\n",
    "        data_dict = {'CP': y_true.to_numpy().reshape(-1, 1)}\n",
    "        savemat('output_file.mat', data_dict)\n",
    "        Cp_min, Cp_mean, Cp_std = eng.Peaks('output_file.mat', nargout=3)\n",
    "        results[tap] = (Cp_min, Cp_mean, Cp_std)\n",
    "\n",
    "    # Convert the results array to a DataFrame\n",
    "    df_min_peaks = pd.DataFrame(results).transpose()\n",
    "    return df_min_peaks\n",
    "\n",
    "# Parameters\n",
    "\n",
    "num_taps = 150  # Assuming there are 150 taps\n",
    "\n",
    "# Compute min peaks for each DataFrame\n",
    "df_stats_0 = compute_min_peaks(df_0, num_taps, eng)\n",
    "df_stats_15 = compute_min_peaks(df_15, num_taps, eng)\n",
    "df_stats_30 = compute_min_peaks(df_30, num_taps, eng)\n",
    "df_stats_45 = compute_min_peaks(df_45, num_taps, eng)\n",
    "df_stats_60 = compute_min_peaks(df_60, num_taps, eng)\n",
    "df_stats_75 = compute_min_peaks(df_75, num_taps, eng)\n",
    "df_stats_90 = compute_min_peaks(df_90, num_taps, eng)\n",
    "\n",
    "df_stats_0.to_pickle('df_entire_stats_0.pkl')\n",
    "df_stats_15.to_pickle('df_entire_stats_15.pkl')\n",
    "df_stats_30.to_pickle('df_entire_stats_30.pkl')\n",
    "df_stats_45.to_pickle('df_entire_stats_45.pkl')\n",
    "df_stats_60.to_pickle('df_entire_stats_60.pkl')\n",
    "df_stats_75.to_pickle('df_entire_stats_75.pkl')\n",
    "df_stats_90.to_pickle('df_entire_stats_90.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = ''\n",
    "\n",
    "df_entire_0 = pd.read_pickle(first + 'df_entire_stats_0.pkl')\n",
    "print(df_entire_0.shape)\n",
    "\n",
    "df_entire_15 = pd.read_pickle(first + 'df_entire_stats_15.pkl')\n",
    "print(df_entire_15.shape)\n",
    "\n",
    "df_entire_30 = pd.read_pickle(first + 'df_entire_stats_30.pkl')\n",
    "print(df_entire_30.shape)\n",
    "\n",
    "df_entire_45 = pd.read_pickle(first + 'df_entire_stats_45.pkl')\n",
    "print(df_entire_45.shape)\n",
    "\n",
    "df_entire_60 = pd.read_pickle(first + 'df_entire_stats_60.pkl')\n",
    "print(df_entire_60.shape)\n",
    "\n",
    "df_entire_75 = pd.read_pickle(first + 'df_entire_stats_75.pkl')\n",
    "print(df_entire_75.shape)\n",
    "\n",
    "df_entire_90 = pd.read_pickle(first + 'df_entire_stats_90.pkl')\n",
    "print(df_entire_90.shape)\n",
    "\n",
    "df_entire_double_0 = pd.read_pickle(first + 'df_entire_double_stats_0.pkl')\n",
    "print(df_entire_0.shape)\n",
    "\n",
    "df_entire_double_15 = pd.read_pickle(first + 'df_entire_double_stats_15.pkl')\n",
    "print(df_entire_15.shape)\n",
    "\n",
    "df_entire_double_30 = pd.read_pickle(first + 'df_entire_double_stats_30.pkl')\n",
    "print(df_entire_30.shape)\n",
    "\n",
    "df_entire_double_45 = pd.read_pickle(first + 'df_entire_double_stats_45.pkl')\n",
    "print(df_entire_45.shape)\n",
    "\n",
    "df_entire_double_60 = pd.read_pickle(first + 'df_entire_double_stats_60.pkl')\n",
    "print(df_entire_60.shape)\n",
    "\n",
    "df_entire_double_75 = pd.read_pickle(first + 'df_entire_double_stats_75.pkl')\n",
    "print(df_entire_75.shape)\n",
    "\n",
    "df_entire_double_90 = pd.read_pickle(first + 'df_entire_double_stats_90.pkl')\n",
    "print(df_entire_90.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations of Pressure Taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal coordinates (from left to right as they appear to increase):\n",
    "horizontal_distances = [2.00, 5.20, 8.40, 12.00, 15.60, 20.11, 25.73, 31.15, 36.77, 42.19]\n",
    "\n",
    "# Vertical coordinates (from bottom to top as they appear to increase):\n",
    "vertical_distances = [2.00, 5.20, 8.15, 11.11, 15.60, 20.55, 25.28, 30.58, 34.72, 39.45, 44.40, 48.89, 51.85, 54.80, 58.00]\n",
    "\n",
    "roof_pressure_tap_coordinates = [(x, y) for x in horizontal_distances for y in vertical_distances]\n",
    "\n",
    "# Horizontal coordinates (from left to right as they appear to increase):\n",
    "horizontal_distances = [0.5, 3.78, 7.06, 10.34]\n",
    "\n",
    "# Vertical coordinates (from top to bottom as they appear to increase):\n",
    "vertical_distances = [6.0, 3.25, 0.5]\n",
    "\n",
    "each_pv_pressure_tap_coordinates = [(x, y) for y in vertical_distances for x in horizontal_distances]\n",
    "\n",
    "pv_pressure_tap_coordinates = []\n",
    "\n",
    "# add (44.90, 8.0) to the coordinates for each pv pressure tap\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 44.90 + 0.20, 4), y + 8.0) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 55.94 + 0.20, 4), y + 8.0) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20,4), y + 8.0) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 55.94 + 0.20, 4), y + 17.45) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 17.45) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 26.89) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 39.34) for x, y in each_pv_pressure_tap_coordinates])\n",
    "pv_pressure_tap_coordinates.append([(np.round(x + 66.98 + 0.20, 4), y + 45.79) for x, y in each_pv_pressure_tap_coordinates])\n",
    "\n",
    "# take arccos of 6.21 / 6.50 to find the angle\n",
    "pv_angle_radian = np.arccos(6.21 / 6.50)\n",
    "pv_angle_degree = np.degrees(pv_angle_radian)\n",
    "pv_pressure_tap_coordinates[0]\n",
    "\n",
    "# Let's create a function to duplicate and concatenate the lists\n",
    "def create_combined_list(pv_panel_lists):\n",
    "    combined_list = []\n",
    "    for panel_list in pv_panel_lists:\n",
    "        for coordinate in panel_list:\n",
    "            # Add the coordinate twice, once for top (blue dot) and once for bottom (red dot)\n",
    "            combined_list.append(coordinate)  # For the blue dot\n",
    "        for coordinate in panel_list:\n",
    "            # Add the coordinate twice, once for top (blue dot) and once for bottom (red dot)\n",
    "            combined_list.append(coordinate)  # For the blue dot\n",
    "    return combined_list\n",
    "\n",
    "# Now we create the combined list\n",
    "enumerated_pv_pressure_tap_coordinates = create_combined_list(pv_pressure_tap_coordinates)\n",
    "\n",
    "# attach pv_pressure_tap_coordinates and combined list\n",
    "pressure_tap_coordinates = roof_pressure_tap_coordinates + enumerated_pv_pressure_tap_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on the Three Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Partially Known Time History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTIAL EXPERIMENTS\n",
    "\n",
    "tap_index = 1\n",
    "taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "\n",
    "df_entire = df_entire_double_45\n",
    "\n",
    "# Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "train_data_first_block = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'block': [1] * len(taps_for_training),\n",
    "    'min_peak': [df_entire.iloc[0, tap - 1][0] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[0, tap - 1][1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[0, tap - 1][2] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "train_data_second_block = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'block': [2] * len(taps_for_training),\n",
    "    'min_peak': [df_entire.iloc[1, tap - 1][0] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[1, tap - 1][1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[1, tap - 1][2] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "# Include the first block of the test tap in the training set\n",
    "tap_index_first_block = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'block': [1],\n",
    "    'min_peak': [df_entire.iloc[0, tap_index-1][0]],\n",
    "    'mean': [df_entire.iloc[0, tap_index-1][1]],\n",
    "    'std': [df_entire.iloc[0, tap_index-1][2]]\n",
    "})\n",
    "\n",
    "train_data = pd.concat([train_data_first_block, train_data_second_block, tap_index_first_block], ignore_index=True)\n",
    "\n",
    "# Extract data for the test tap (tap_index is 1-based, so we use tap_index-1 for 0-based indexing)\n",
    "test_data = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'block': [2],\n",
    "    'min_peak': [df_entire.iloc[1, tap_index-1][0]],  # Second block of tap 1\n",
    "    'mean': [df_entire.iloc[1, tap_index-1][1]],\n",
    "    'std': [df_entire.iloc[1, tap_index-1][2]]\n",
    "})\n",
    "\n",
    "# Extract training features and labels\n",
    "X_train = train_data[['x', 'y', 'block']]\n",
    "X_test = test_data[['x', 'y', 'block']]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "    'GaussianProcess': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100, 150], 'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2], 'max_depth': [2, 3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5, len(taps_for_training)], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10, 1e-12],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store predictions and errors\n",
    "best_predictions = {}\n",
    "best_errors = {}\n",
    "best_model = {}\n",
    "best_params = {}\n",
    "\n",
    "# Perform regression for each statistic and each model\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    y_train = train_data[stat]\n",
    "    y_true = test_data[stat].values[0]\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_pred = None\n",
    "    best_model_name = None\n",
    "    best_model_params = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=5)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        y_pred = grid_search.predict(X_test_scaled)[0]\n",
    "\n",
    "        percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "        if percentage_error < min_error:\n",
    "            min_error = percentage_error\n",
    "            best_pred = y_pred\n",
    "            best_model_name = model_name\n",
    "            best_model_params = grid_search.best_params_\n",
    "\n",
    "    best_predictions[stat] = best_pred\n",
    "    best_errors[stat] = min_error\n",
    "    best_model[stat] = best_model_name\n",
    "    best_params[stat] = best_model_params\n",
    "\n",
    "# Calculate and display percentage errors\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    print(f'{stat} - True value: {test_data[stat].values[0]}, Predicted value: {best_predictions[stat]}, Percentage error: {best_errors[stat]:.2f}%, Best model: {best_model[stat]}, Best parameters: {best_params[stat]}')\n",
    "\n",
    "# Compute the average percentage error\n",
    "average_error = sum(best_errors.values()) / len(best_errors)\n",
    "print(f'Average Percentage Error: {average_error:.2f}%')\n",
    "\n",
    "# Displaying the results in a tabular format\n",
    "results_df = pd.DataFrame({\n",
    "    'Statistic': ['min_peak', 'mean', 'std'],\n",
    "    'True Value': [test_data[stat].values[0] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Predicted Value': [best_predictions[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Percentage Error': [best_errors[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Model': [best_model[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Parameters': [best_params[stat] for stat in ['min_peak', 'mean', 'std']]\n",
    "})\n",
    "\n",
    "# Display the DataFrame using Pandas\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTIAL ALL ANGLES EXPERIMENT\n",
    "\n",
    "# Define angles to iterate over\n",
    "angles = [0, 15, 30, 45, 60, 75, 90]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (50,50), (100,)], 'alpha': [0.0001, 0.001], 'solver': ['adam'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize lists to store errors and best models for each angle\n",
    "angle_errors = []\n",
    "best_models_info = []\n",
    "\n",
    "# Loop through each angle\n",
    "for angle in angles:\n",
    "    # Set the appropriate data frame for the given angle\n",
    "    df_entire = globals()[f'df_entire_double_{angle}']\n",
    "\n",
    "    tap_index = 1\n",
    "    taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "\n",
    "    tap_index = 61\n",
    "    taps_for_training = [46, 47, 48, 62, 63, 76, 77, 78]\n",
    "\n",
    "    tap_index = 81\n",
    "    taps_for_training = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "\n",
    "    # Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "    train_data_first_block = pd.DataFrame({\n",
    "        'tap': taps_for_training,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "        'block': [1] * len(taps_for_training),\n",
    "        'min_peak': [df_entire.iloc[0, tap - 1][0] for tap in taps_for_training],\n",
    "        'mean': [df_entire.iloc[0, tap - 1][1] for tap in taps_for_training],\n",
    "        'std': [df_entire.iloc[0, tap - 1][2] for tap in taps_for_training]\n",
    "    })\n",
    "\n",
    "    train_data_second_block = pd.DataFrame({\n",
    "        'tap': taps_for_training,\n",
    "        'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "        'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "        'block': [2] * len(taps_for_training),\n",
    "        'min_peak': [df_entire.iloc[1, tap - 1][0] for tap in taps_for_training],\n",
    "        'mean': [df_entire.iloc[1, tap - 1][1] for tap in taps_for_training],\n",
    "        'std': [df_entire.iloc[1, tap - 1][2] for tap in taps_for_training]\n",
    "    })\n",
    "\n",
    "    # Include the first block of the test tap in the training set\n",
    "    tap_index_first_block = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'block': [1],\n",
    "        'min_peak': [df_entire.iloc[0, tap_index-1][0]],\n",
    "        'mean': [df_entire.iloc[0, tap_index-1][1]],\n",
    "        'std': [df_entire.iloc[0, tap_index-1][2]]\n",
    "    })\n",
    "\n",
    "    train_data = pd.concat([train_data_first_block, train_data_second_block, tap_index_first_block], ignore_index=True)\n",
    "\n",
    "    # Extract data for the test tap (tap_index is 1-based, so we use tap_index-1 for 0-based indexing)\n",
    "    test_data = pd.DataFrame({\n",
    "        'tap': [tap_index],\n",
    "        'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "        'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "        'block': [2],\n",
    "        'min_peak': [df_entire.iloc[1, tap_index-1][0]],  # Second block of tap 1\n",
    "        'mean': [df_entire.iloc[1, tap_index-1][1]],\n",
    "        'std': [df_entire.iloc[1, tap_index-1][2]]\n",
    "    })\n",
    "\n",
    "    # Extract training features and labels\n",
    "    X_train = train_data[['x', 'y', 'block']]\n",
    "    X_test = test_data[['x', 'y', 'block']]\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeRegressor(),\n",
    "        'RandomForest': RandomForestRegressor(),\n",
    "        'GradientBoosting': GradientBoostingRegressor(),\n",
    "        'KNeighbors': KNeighborsRegressor(),\n",
    "        'NeuralNetwork': MLPRegressor(max_iter=3000),\n",
    "        'GaussianProcess': GaussianProcessRegressor()\n",
    "    }\n",
    "\n",
    "    # Initialize dictionaries to store predictions and errors\n",
    "    best_predictions = {}\n",
    "    best_errors = {}\n",
    "    best_model = {}\n",
    "    best_params = {}\n",
    "\n",
    "    # Perform regression for each statistic and each model\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        y_train = train_data[stat]\n",
    "        y_true = test_data[stat].values[0]\n",
    "\n",
    "        min_error = float('inf')\n",
    "        best_pred = None\n",
    "        best_model_name = None\n",
    "        best_model_params = None\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            grid_search = GridSearchCV(model, param_grids[model_name], cv=5)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "            y_pred = grid_search.predict(X_test_scaled)[0]\n",
    "\n",
    "            percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "            if percentage_error < min_error:\n",
    "                min_error = percentage_error\n",
    "                best_pred = y_pred\n",
    "                best_model_name = model_name\n",
    "                best_model_params = grid_search.best_params_\n",
    "\n",
    "        best_predictions[stat] = best_pred\n",
    "        best_errors[stat] = min_error\n",
    "        best_model[stat] = best_model_name\n",
    "        best_params[stat] = best_model_params\n",
    "\n",
    "    # Store the errors and best model info for this angle\n",
    "    angle_errors.append({\n",
    "        'Angle': angle,\n",
    "        'Min Peak Error': best_errors['min_peak'],\n",
    "        'Mean Error': best_errors['mean'],\n",
    "        'Std Error': best_errors['std']\n",
    "    })\n",
    "    \n",
    "    best_models_info.append({\n",
    "        'Angle': angle,\n",
    "        'Min Peak Best Model': best_model['min_peak'],\n",
    "        'Min Peak Best Params': best_params['min_peak'],\n",
    "        'Mean Best Model': best_model['mean'],\n",
    "        'Mean Best Params': best_params['mean'],\n",
    "        'Std Best Model': best_model['std'],\n",
    "        'Std Best Params': best_params['std']\n",
    "    })\n",
    "\n",
    "# Create DataFrames to display the errors and best model info for each angle\n",
    "errors_df = pd.DataFrame(angle_errors)\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "\n",
    "# Display the DataFrames using Pandas\n",
    "print(errors_df)\n",
    "print(best_models_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Completely Unknown Time History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Experiments \n",
    "\n",
    "taps_for_training = [65, 66, 67, 80, 82, 95, 96, 97]\n",
    "tap_index = 81\n",
    "\n",
    "taps_for_training = [46, 47, 48, 62, 63, 76, 77, 78]\n",
    "tap_index = 61\n",
    "\n",
    "taps_for_training = [2, 16, 17, 3, 18, 31, 32, 33]\n",
    "\n",
    "tap_index = 1\n",
    "\n",
    "df_entire = df_entire_0\n",
    "\n",
    "# Extract data for the specified taps (index -1 because it's zero-indexed)\n",
    "train_data = pd.DataFrame({\n",
    "    'tap': taps_for_training,\n",
    "    'x': [roof_pressure_tap_coordinates[tap - 1][0] for tap in taps_for_training],\n",
    "    'y': [roof_pressure_tap_coordinates[tap - 1][1] for tap in taps_for_training],\n",
    "    'min_peak': [df_entire.iloc[0, tap - 1] for tap in taps_for_training],\n",
    "    'mean': [df_entire.iloc[1, tap - 1] for tap in taps_for_training],\n",
    "    'std': [df_entire.iloc[2, tap - 1] for tap in taps_for_training]\n",
    "})\n",
    "\n",
    "# Extract data for tap 62 (index 61 because it's zero-indexed)\n",
    "test_data = pd.DataFrame({\n",
    "    'tap': [tap_index],\n",
    "    'x': [roof_pressure_tap_coordinates[tap_index-1][0]],\n",
    "    'y': [roof_pressure_tap_coordinates[tap_index-1][1]],\n",
    "    'min_peak': [df_entire.iloc[0, tap_index-1]],\n",
    "    'mean': [df_entire.iloc[1, tap_index-1]],\n",
    "    'std': [df_entire.iloc[2, tap_index-1]]\n",
    "})\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[['x', 'y']])\n",
    "X_test_scaled = scaler.transform(test_data[['x', 'y']])\n",
    "\n",
    "'''# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'RandomForest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.05, 0.1, 0.2], 'max_depth': [3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [3, len(taps_for_training)], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [2000]},\n",
    "    'GaussianProcess': {'kernel': [ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\") , ConstantKernel(1.0, constant_value_bounds=\"fixed\") * Matern(1.0, length_scale_bounds=\"fixed\") , ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RationalQuadratic(1.0, length_scale_bounds=\"fixed\")]}\n",
    "}'''\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'DecisionTree': {'max_depth': [None, 5, 10, 25], 'min_samples_split': [2, 5, 10, 20], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error']},\n",
    "    'RandomForest': {'n_estimators': [25, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [10, 25, 50, 100, 150], 'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.2], 'max_depth': [2, 3, 5, 10]},\n",
    "    'KNeighbors': {'n_neighbors': [2, 3, 5, len(taps_for_training)], 'weights': ['uniform', 'distance'], 'p': [1, 2]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], 'alpha': [0.0001, 0.001, 0.01], 'solver': ['adam', 'lbfgs'], 'max_iter': [3000]},\n",
    "    'GaussianProcess': {\n",
    "        'kernel': [\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)),\n",
    "            C(1.0, constant_value_bounds=(1e-2, 1e2)) * RationalQuadratic(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        ],\n",
    "        'alpha': [1e-2, 1e-5, 1e-10, 1e-12],\n",
    "        'normalize_y': [False, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'NeuralNetwork': MLPRegressor(),\n",
    "    'GaussianProcess': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store predictions, errors, and best parameters\n",
    "best_predictions = {}\n",
    "best_errors = {}\n",
    "best_model_instance = {}\n",
    "best_params = {}\n",
    "\n",
    "# Perform regression for each statistic and each model\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    y_train = train_data[stat]\n",
    "    y_true = test_data[stat].values[0]\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_pred = None\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_param = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Use the best model found\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)[0]\n",
    "\n",
    "        percentage_error = 100 * abs((y_true - y_pred) / y_true)\n",
    "\n",
    "        # Store the best parameters and errors for each model\n",
    "        if model_name not in best_params:\n",
    "            best_params[model_name] = {}\n",
    "        best_params[model_name][stat] = {\n",
    "            'params': grid_search.best_params_,\n",
    "            'error': percentage_error,\n",
    "            'pred': y_pred\n",
    "        }\n",
    "\n",
    "        if percentage_error < min_error:\n",
    "            min_error = percentage_error\n",
    "            best_pred = y_pred\n",
    "            best_model_name = model_name\n",
    "            best_param = grid_search.best_params_\n",
    "\n",
    "    best_predictions[stat] = best_pred\n",
    "    best_errors[stat] = min_error\n",
    "    best_model_instance[stat] = models[best_model_name]\n",
    "\n",
    "# Calculate and display percentage errors\n",
    "for stat in ['min_peak', 'mean', 'std']:\n",
    "    print(f'{stat} - True value: {test_data[stat].values[0]}, Predicted value: {best_predictions[stat]}, Percentage error: {best_errors[stat]:.2f}%, Best model: {best_model_instance[stat]}')\n",
    "\n",
    "# Displaying the results in a tabular format\n",
    "results_df = pd.DataFrame({\n",
    "    'Statistic': ['min_peak', 'mean', 'std'],\n",
    "    'True Value': [test_data[stat].values[0] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Predicted Value': [best_predictions[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Percentage Error': [best_errors[stat] for stat in ['min_peak', 'mean', 'std']],\n",
    "    'Best Model': [type(best_model_instance[stat]).__name__ for stat in ['min_peak', 'mean', 'std']]\n",
    "})\n",
    "\n",
    "# Display the DataFrame using Pandas\n",
    "print(results_df)\n",
    "\n",
    "# Print the best parameters and errors for each model\n",
    "for model_name in best_params:\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    for stat in ['min_peak', 'mean', 'std']:\n",
    "        print(f'  Statistic: {stat}')\n",
    "        print(f'    Best Params: {best_params[model_name][stat][\"params\"]}')\n",
    "        print(f'    Predicted Value: {best_params[model_name][stat][\"pred\"]}')\n",
    "        print(f'    Percentage Error: {best_params[model_name][stat][\"error\"]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Statistics of a Missing Wind Angle Time History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on Pressure Coefficients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
